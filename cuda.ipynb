{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cuda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7sxrUVwbvE"
      },
      "source": [
        "# install cuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrGbdFknzBxw"
      },
      "source": [
        "!apt-get --purge remove cuda nvidia* libnvidia-*\n",
        "!dpkg -l | grep cuda- | awk '{print $2}' | xargs -n1 dpkg --purge\n",
        "!apt-get remove cuda-*\n",
        "!apt autoremove\n",
        "!apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5OsjewlzCxh"
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.2/Prod/local_installers/cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64 -O cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-2-local_9.2.88-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-2-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda-9.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc4xJhuB0tAo"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCiXSorZwS7l"
      },
      "source": [
        "## colab magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p4CZNIf1B8V"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvTL0DLr03LG"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku3y47JywWaR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh060kXTVEjh"
      },
      "source": [
        "!apt-get install nvidia-cuda-toolkit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXVW2ae5wF_N"
      },
      "source": [
        "## symbolic link "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYWQGQWaVOZV"
      },
      "source": [
        "! ln -s /usr/bin/gcc-6 /usr/local/cuda/bin/gcc\n",
        "! ln -s /usr/bin/g++-6 /usr/local/cuda/bin/g++"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo92b6nYwM8w"
      },
      "source": [
        "## add path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDXXismyG_Iz"
      },
      "source": [
        "!export CUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-9.2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqTn_XgPrDkE"
      },
      "source": [
        "# Cuda hello world example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRP3Ho5i08ke",
        "outputId": "0b285f14-69a7-4cac-fde1-835b32b67b38"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "    int\n",
        "    main()\n",
        "{\n",
        "    std::cout << \"Good afternoon! \\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good afternoon! \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHoIWVcuUiC"
      },
      "source": [
        "# C to Cuda (add vector example) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX8lbrUHuVxo"
      },
      "source": [
        "## C code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-hAET_VhgJw",
        "outputId": "178ad05c-2de6-4fa9-8e67-59d6338e36cf"
      },
      "source": [
        "%%writefile VectorAdd.c\n",
        "\n",
        "#include <stdio.h>\n",
        "#define SIZE\t1024\n",
        "\n",
        "void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tint i;\n",
        "\n",
        "\tfor (i=0; i < n; ++i)\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tint *a, *b, *c;\n",
        "\t\n",
        "\ta = (int *)malloc(SIZE * sizeof(int));\n",
        "\tb = (int *)malloc(SIZE * sizeof(int));\n",
        "\tc = (int *)malloc(SIZE * sizeof(int));\n",
        "\t\n",
        "\tfor (int i = 0; i < SIZE; ++i)\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\t\n",
        "\tVectorAdd(a, b, c, SIZE);\n",
        "\n",
        "\tfor (int i = 0; i < 10; ++i)\n",
        "\t\tprintf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\n",
        "\tfree(a);\n",
        "\tfree(b);\n",
        "\tfree(c);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing VectorAdd.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfMa13mjuZQC"
      },
      "source": [
        "## cuda code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCOPKZyVhll0",
        "outputId": "d7b22fae-98f7-4927-b5b0-30f9e86581ac"
      },
      "source": [
        "%%writefile VectorAdd.cu\n",
        "#include <stdio.h>\n",
        "#define SIZE\t1024\n",
        "\n",
        "__global__ void VectorAdd(int *a, int *b, int *c, int n)\n",
        "{\n",
        "\tint i = threadIdx.x;\n",
        "\n",
        "\tif (i < n)\n",
        "\t\tc[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "\tint *a, *b, *c;\n",
        "\t\n",
        "\tcudaMallocManaged(&a, SIZE*sizeof(int));\n",
        "\tcudaMallocManaged(&b, SIZE*sizeof(int));\n",
        "\tcudaMallocManaged(&c, SIZE*sizeof(int));\n",
        "\t\n",
        "\tfor (int i = 0; i < SIZE; ++i)\n",
        "\t{\n",
        "\t\ta[i] = i;\n",
        "\t\tb[i] = i;\n",
        "\t\tc[i] = 0;\n",
        "\t}\n",
        "\t\n",
        "\tVectorAdd <<<1,SIZE>>>(a, b, c, SIZE);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "\tfor (int i = 0; i < 10; ++i)\n",
        "\t\tprintf(\"c[%d] = %d\\n\", i, c[i]);\n",
        "\n",
        "\tcudaFree(a);\n",
        "\tcudaFree(b);\n",
        "\tcudaFree(c);\n",
        "\n",
        "\treturn 0;\n",
        "}"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing VectorAdd.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QifD5Wzcv5ml"
      },
      "source": [
        "## compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvobJWhUv4EM"
      },
      "source": [
        "!nvcc VectorAdd.cu"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmuY7ZCBwAWA"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94sEO0RJv_8t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPNdkIcdtMPB"
      },
      "source": [
        "## Sample: matrixMul\n",
        "## Minimum spec: SM 3.5\n",
        "\n",
        "This sample implements matrix multiplication which makes use of shared memory to ensure data reuse, the matrix multiplication is done using tiling approach. It has been written for clarity of exposition to illustrate various CUDA programming principles, not with the goal of providing the most performant generic kernel for matrix multiplication.\n",
        "\n",
        "Key concepts:\n",
        "CUDA Runtime API\n",
        "Linear Algebra\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5RtICs8tLwg",
        "outputId": "a545e47b-5ffd-4edc-ee2a-118306f95659"
      },
      "source": [
        "%%writefile matrixMul.cu\n",
        "\n",
        "/**\n",
        " * Copyright 1993-2015 NVIDIA Corporation.  All rights reserved.\n",
        " *\n",
        " * Please refer to the NVIDIA end user license agreement (EULA) associated\n",
        " * with this source code for terms and conditions that govern your use of\n",
        " * this software. Any use, reproduction, disclosure, or distribution of\n",
        " * this software and related documentation outside the terms of the EULA\n",
        " * is strictly prohibited.\n",
        " *\n",
        " */\n",
        "\n",
        "/**\n",
        " * Matrix multiplication: C = A * B.\n",
        " * Host code.\n",
        " *\n",
        " * This sample implements matrix multiplication which makes use of shared memory\n",
        " * to ensure data reuse, the matrix multiplication is done using tiling\n",
        " * approach. It has been written for clarity of exposition to illustrate various\n",
        " * CUDA programming principles, not with the goal of providing the most\n",
        " * performant generic kernel for matrix multiplication. See also: V. Volkov and\n",
        " * J. Demmel, \"Benchmarking GPUs to tune dense linear algebra,\" in Proc. 2008\n",
        " * ACM/IEEE Conf. on Supercomputing (SC '08), Piscataway, NJ: IEEE Press, 2008,\n",
        " * pp. Art. 31:1-11.\n",
        " */\n",
        "\n",
        "// System includes\n",
        "#include <assert.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// CUDA runtime\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// Helper functions and utilities to work with CUDA\n",
        "#include <helper_cuda.h>\n",
        "#include <helper_functions.h>\n",
        "\n",
        "/**\n",
        " * Matrix multiplication (CUDA Kernel) on the device: C = A * B\n",
        " * wA is A's width and wB is B's width\n",
        " */\n",
        "template <int BLOCK_SIZE>\n",
        "__global__ void MatrixMulCUDA(float *C, float *A, float *B, int wA, int wB) {\n",
        "  // Block index\n",
        "  int bx = blockIdx.x;\n",
        "  int by = blockIdx.y;\n",
        "\n",
        "  // Thread index\n",
        "  int tx = threadIdx.x;\n",
        "  int ty = threadIdx.y;\n",
        "\n",
        "  // Index of the first sub-matrix of A processed by the block\n",
        "  int aBegin = wA * BLOCK_SIZE * by;\n",
        "\n",
        "  // Index of the last sub-matrix of A processed by the block\n",
        "  int aEnd = aBegin + wA - 1;\n",
        "\n",
        "  // Step size used to iterate through the sub-matrices of A\n",
        "  int aStep = BLOCK_SIZE;\n",
        "\n",
        "  // Index of the first sub-matrix of B processed by the block\n",
        "  int bBegin = BLOCK_SIZE * bx;\n",
        "\n",
        "  // Step size used to iterate through the sub-matrices of B\n",
        "  int bStep = BLOCK_SIZE * wB;\n",
        "\n",
        "  // Csub is used to store the element of the block sub-matrix\n",
        "  // that is computed by the thread\n",
        "  float Csub = 0;\n",
        "\n",
        "  // Loop over all the sub-matrices of A and B\n",
        "  // required to compute the block sub-matrix\n",
        "  for (int a = aBegin, b = bBegin; a <= aEnd; a += aStep, b += bStep) {\n",
        "    // Declaration of the shared memory array As used to\n",
        "    // store the sub-matrix of A\n",
        "    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Declaration of the shared memory array Bs used to\n",
        "    // store the sub-matrix of B\n",
        "    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n",
        "\n",
        "    // Load the matrices from device memory\n",
        "    // to shared memory; each thread loads\n",
        "    // one element of each matrix\n",
        "    As[ty][tx] = A[a + wA * ty + tx];\n",
        "    Bs[ty][tx] = B[b + wB * ty + tx];\n",
        "\n",
        "    // Synchronize to make sure the matrices are loaded\n",
        "    __syncthreads();\n",
        "\n",
        "    // Multiply the two matrices together;\n",
        "    // each thread computes one element\n",
        "    // of the block sub-matrix\n",
        "#pragma unroll\n",
        "\n",
        "    for (int k = 0; k < BLOCK_SIZE; ++k) {\n",
        "      Csub += As[ty][k] * Bs[k][tx];\n",
        "    }\n",
        "\n",
        "    // Synchronize to make sure that the preceding\n",
        "    // computation is done before loading two new\n",
        "    // sub-matrices of A and B in the next iteration\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // Write the block sub-matrix to device memory;\n",
        "  // each thread writes one element\n",
        "  int c = wB * BLOCK_SIZE * by + BLOCK_SIZE * bx;\n",
        "  C[c + wB * ty + tx] = Csub;\n",
        "}\n",
        "\n",
        "void ConstantInit(float *data, int size, float val) {\n",
        "  for (int i = 0; i < size; ++i) {\n",
        "    data[i] = val;\n",
        "  }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Run a simple test of matrix multiplication using CUDA\n",
        " */\n",
        "int MatrixMultiply(int argc, char **argv, int block_size, const dim3 &dimsA,\n",
        "                   const dim3 &dimsB) {\n",
        "  // Allocate host memory for matrices A and B\n",
        "  unsigned int size_A = dimsA.x * dimsA.y;\n",
        "  unsigned int mem_size_A = sizeof(float) * size_A;\n",
        "  float *h_A;\n",
        "  checkCudaErrors(cudaMallocHost(&h_A, mem_size_A));\n",
        "  unsigned int size_B = dimsB.x * dimsB.y;\n",
        "  unsigned int mem_size_B = sizeof(float) * size_B;\n",
        "  float *h_B;\n",
        "  checkCudaErrors(cudaMallocHost(&h_B, mem_size_B));\n",
        "  cudaStream_t stream;\n",
        "\n",
        "  // Initialize host memory\n",
        "  const float valB = 0.01f;\n",
        "  ConstantInit(h_A, size_A, 1.0f);\n",
        "  ConstantInit(h_B, size_B, valB);\n",
        "\n",
        "  // Allocate device memory\n",
        "  float *d_A, *d_B, *d_C;\n",
        "\n",
        "  // Allocate host matrix C\n",
        "  dim3 dimsC(dimsB.x, dimsA.y, 1);\n",
        "  unsigned int mem_size_C = dimsC.x * dimsC.y * sizeof(float);\n",
        "  float *h_C;\n",
        "  checkCudaErrors(cudaMallocHost(&h_C, mem_size_C));\n",
        "\n",
        "  if (h_C == NULL) {\n",
        "    fprintf(stderr, \"Failed to allocate host matrix C!\\n\");\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_A), mem_size_A));\n",
        "  checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_B), mem_size_B));\n",
        "  checkCudaErrors(cudaMalloc(reinterpret_cast<void **>(&d_C), mem_size_C));\n",
        "  // Allocate CUDA events that we'll use for timing\n",
        "  cudaEvent_t start, stop;\n",
        "  checkCudaErrors(cudaEventCreate(&start));\n",
        "  checkCudaErrors(cudaEventCreate(&stop));\n",
        "\n",
        "  checkCudaErrors(cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking));\n",
        "\n",
        "  // copy host memory to device\n",
        "  checkCudaErrors(\n",
        "      cudaMemcpyAsync(d_A, h_A, mem_size_A, cudaMemcpyHostToDevice, stream));\n",
        "  checkCudaErrors(\n",
        "      cudaMemcpyAsync(d_B, h_B, mem_size_B, cudaMemcpyHostToDevice, stream));\n",
        "\n",
        "  // Setup execution parameters\n",
        "  dim3 threads(block_size, block_size);\n",
        "  dim3 grid(dimsB.x / threads.x, dimsA.y / threads.y);\n",
        "\n",
        "  // Create and start timer\n",
        "  printf(\"Computing result using CUDA Kernel...\\n\");\n",
        "\n",
        "  // Performs warmup operation using matrixMul CUDA kernel\n",
        "  if (block_size == 16) {\n",
        "    MatrixMulCUDA<16>\n",
        "        <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);\n",
        "  } else {\n",
        "    MatrixMulCUDA<32>\n",
        "        <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);\n",
        "  }\n",
        "\n",
        "  printf(\"done\\n\");\n",
        "  checkCudaErrors(cudaStreamSynchronize(stream));\n",
        "\n",
        "  // Record the start event\n",
        "  checkCudaErrors(cudaEventRecord(start, stream));\n",
        "\n",
        "  // Execute the kernel\n",
        "  int nIter = 300;\n",
        "\n",
        "  for (int j = 0; j < nIter; j++) {\n",
        "    if (block_size == 16) {\n",
        "      MatrixMulCUDA<16>\n",
        "          <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);\n",
        "    } else {\n",
        "      MatrixMulCUDA<32>\n",
        "          <<<grid, threads, 0, stream>>>(d_C, d_A, d_B, dimsA.x, dimsB.x);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Record the stop event\n",
        "  checkCudaErrors(cudaEventRecord(stop, stream));\n",
        "\n",
        "  // Wait for the stop event to complete\n",
        "  checkCudaErrors(cudaEventSynchronize(stop));\n",
        "\n",
        "  float msecTotal = 0.0f;\n",
        "  checkCudaErrors(cudaEventElapsedTime(&msecTotal, start, stop));\n",
        "\n",
        "  // Compute and print the performance\n",
        "  float msecPerMatrixMul = msecTotal / nIter;\n",
        "  double flopsPerMatrixMul = 2.0 * static_cast<double>(dimsA.x) *\n",
        "                             static_cast<double>(dimsA.y) *\n",
        "                             static_cast<double>(dimsB.x);\n",
        "  double gigaFlops =\n",
        "      (flopsPerMatrixMul * 1.0e-9f) / (msecPerMatrixMul / 1000.0f);\n",
        "  printf(\n",
        "      \"Performance= %.2f GFlop/s, Time= %.3f msec, Size= %.0f Ops,\"\n",
        "      \" WorkgroupSize= %u threads/block\\n\",\n",
        "      gigaFlops, msecPerMatrixMul, flopsPerMatrixMul, threads.x * threads.y);\n",
        "\n",
        "  // Copy result from device to host\n",
        "  checkCudaErrors(\n",
        "      cudaMemcpyAsync(h_C, d_C, mem_size_C, cudaMemcpyDeviceToHost, stream));\n",
        "  checkCudaErrors(cudaStreamSynchronize(stream));\n",
        "\n",
        "  printf(\"Checking computed result for correctness: \");\n",
        "  bool correct = true;\n",
        "\n",
        "  // test relative error by the formula\n",
        "  //     |<x, y>_cpu - <x,y>_gpu|/<|x|, |y|>  < eps\n",
        "  double eps = 1.e-6;  // machine zero\n",
        "\n",
        "  for (int i = 0; i < static_cast<int>(dimsC.x * dimsC.y); i++) {\n",
        "    double abs_err = fabs(h_C[i] - (dimsA.x * valB));\n",
        "    double dot_length = dimsA.x;\n",
        "    double abs_val = fabs(h_C[i]);\n",
        "    double rel_err = abs_err / abs_val / dot_length;\n",
        "\n",
        "    if (rel_err > eps) {\n",
        "      printf(\"Error! Matrix[%05d]=%.8f, ref=%.8f error term is > %E\\n\", i,\n",
        "             h_C[i], dimsA.x * valB, eps);\n",
        "      correct = false;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  printf(\"%s\\n\", correct ? \"Result = PASS\" : \"Result = FAIL\");\n",
        "\n",
        "  // Clean up memory\n",
        "  checkCudaErrors(cudaFreeHost(h_A));\n",
        "  checkCudaErrors(cudaFreeHost(h_B));\n",
        "  checkCudaErrors(cudaFreeHost(h_C));\n",
        "  checkCudaErrors(cudaFree(d_A));\n",
        "  checkCudaErrors(cudaFree(d_B));\n",
        "  checkCudaErrors(cudaFree(d_C));\n",
        "  checkCudaErrors(cudaEventDestroy(start));\n",
        "  checkCudaErrors(cudaEventDestroy(stop));\n",
        "  printf(\n",
        "      \"\\nNOTE: The CUDA Samples are not meant for performance\"\n",
        "      \"measurements. Results may vary when GPU Boost is enabled.\\n\");\n",
        "\n",
        "  if (correct) {\n",
        "    return EXIT_SUCCESS;\n",
        "  } else {\n",
        "    return EXIT_FAILURE;\n",
        "  }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Program main\n",
        " */\n",
        "int main(int argc, char **argv) {\n",
        "  printf(\"[Matrix Multiply Using CUDA] - Starting...\\n\");\n",
        "\n",
        "  if (checkCmdLineFlag(argc, (const char **)argv, \"help\") ||\n",
        "      checkCmdLineFlag(argc, (const char **)argv, \"?\")) {\n",
        "    printf(\"Usage -device=n (n >= 0 for deviceID)\\n\");\n",
        "    printf(\"      -wA=WidthA -hA=HeightA (Width x Height of Matrix A)\\n\");\n",
        "    printf(\"      -wB=WidthB -hB=HeightB (Width x Height of Matrix B)\\n\");\n",
        "    printf(\n",
        "        \"  Note: Outer matrix dimensions of A & B matrices\"\n",
        "        \" must be equal.\\n\");\n",
        "\n",
        "    exit(EXIT_SUCCESS);\n",
        "  }\n",
        "\n",
        "  // This will pick the best possible CUDA capable device, otherwise\n",
        "  // override the device ID based on input provided at the command line\n",
        "  int dev = findCudaDevice(argc, (const char **)argv);\n",
        "\n",
        "  int block_size = 32;\n",
        "\n",
        "  dim3 dimsA(5 * 2 * block_size, 5 * 2 * block_size, 1);\n",
        "  dim3 dimsB(5 * 4 * block_size, 5 * 2 * block_size, 1);\n",
        "\n",
        "  // width of Matrix A\n",
        "  if (checkCmdLineFlag(argc, (const char **)argv, \"wA\")) {\n",
        "    dimsA.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wA\");\n",
        "  }\n",
        "\n",
        "  // height of Matrix A\n",
        "  if (checkCmdLineFlag(argc, (const char **)argv, \"hA\")) {\n",
        "    dimsA.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hA\");\n",
        "  }\n",
        "\n",
        "  // width of Matrix B\n",
        "  if (checkCmdLineFlag(argc, (const char **)argv, \"wB\")) {\n",
        "    dimsB.x = getCmdLineArgumentInt(argc, (const char **)argv, \"wB\");\n",
        "  }\n",
        "\n",
        "  // height of Matrix B\n",
        "  if (checkCmdLineFlag(argc, (const char **)argv, \"hB\")) {\n",
        "    dimsB.y = getCmdLineArgumentInt(argc, (const char **)argv, \"hB\");\n",
        "  }\n",
        "\n",
        "  if (dimsA.x != dimsB.y) {\n",
        "    printf(\"Error: outer matrix dimensions must be equal. (%d != %d)\\n\",\n",
        "           dimsA.x, dimsB.y);\n",
        "    exit(EXIT_FAILURE);\n",
        "  }\n",
        "\n",
        "  printf(\"MatrixA(%d,%d), MatrixB(%d,%d)\\n\", dimsA.x, dimsA.y, dimsB.x,\n",
        "         dimsB.y);\n",
        "\n",
        "  int matrix_result = MatrixMultiply(argc, argv, block_size, dimsA, dimsB);\n",
        "\n",
        "  exit(matrix_result);\n",
        "}"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrixMul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwCwyNjlvMuK"
      },
      "source": [
        "## code dependency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq_sFcy7ZUaB",
        "outputId": "a3f84c75-2bfe-4d47-ad42-799290f172aa"
      },
      "source": [
        "!ls /usr/local/cuda/samples/common/inc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda_drvapi_dynlink.c  helper_gl.h\t nvVector.h\n",
            "drvapi_error_string.h  helper_image.h\t paramgl.h\n",
            "dynlink\t\t       helper_math.h\t param.h\n",
            "dynlink_d3d10.h        helper_string.h\t rendercheck_d3d10.h\n",
            "dynlink_d3d11.h        helper_timer.h\t rendercheck_d3d11.h\n",
            "exception.h\t       multithreading.h  rendercheck_d3d9.h\n",
            "GL\t\t       nvMath.h\t\t rendercheck_gles.h\n",
            "helper_cuda_drvapi.h   nvMatrix.h\t rendercheck_gl.h\n",
            "helper_cuda.h\t       nvQuaternion.h\t timer.h\n",
            "helper_cusolver.h      nvrtc_helper.h\n",
            "helper_functions.h     nvShaderUtils.h\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9qxKf1Ordlc"
      },
      "source": [
        "# compile and run from command-line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWBRKrJ-Wgc8"
      },
      "source": [
        "!nvcc matrixMul.cu -I/usr/local/cuda/samples/common/inc"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d0Dz0o5ajHH",
        "outputId": "cd487faa-4c82-4626-ef3f-7b045dec3041"
      },
      "source": [
        "!./a.out"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Matrix Multiply Using CUDA] - Starting...\n",
            "GPU Device 0: \"Tesla K80\" with compute capability 3.7\n",
            "\n",
            "MatrixA(320,320), MatrixB(640,320)\n",
            "Computing result using CUDA Kernel...\n",
            "done\n",
            "Performance= 225.61 GFlop/s, Time= 0.581 msec, Size= 131072000 Ops, WorkgroupSize= 1024 threads/block\n",
            "Checking computed result for correctness: Result = PASS\n",
            "\n",
            "NOTE: The CUDA Samples are not meant for performancemeasurements. Results may vary when GPU Boost is enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxUYajCXrYIs"
      },
      "source": [
        "# cuda samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ipVjj_adNv7",
        "outputId": "5c40bf7e-1570-4b95-a4fe-4b3e538f11a1"
      },
      "source": [
        "!ls /usr/local/cuda/samples/0_Simple/"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "asyncAPI\t    simpleAssert\t\t  simpleSeparateCompilation\n",
            "cdpSimplePrint\t    simpleAssert_nvrtc\t\t  simpleStreams\n",
            "cdpSimpleQuicksort  simpleAtomicIntrinsics\t  simpleSurfaceWrite\n",
            "clock\t\t    simpleAtomicIntrinsics_nvrtc  simpleTemplates\n",
            "clock_nvrtc\t    simpleCallback\t\t  simpleTemplates_nvrtc\n",
            "cppIntegration\t    simpleCooperativeGroups\t  simpleTexture\n",
            "cppOverload\t    simpleCubemapTexture\t  simpleTextureDrv\n",
            "cudaOpenMP\t    simpleIPC\t\t\t  simpleVoteIntrinsics\n",
            "cudaTensorCoreGemm  simpleLayeredTexture\t  simpleVoteIntrinsics_nvrtc\n",
            "fp16ScalarProduct   simpleMPI\t\t\t  simpleZeroCopy\n",
            "inlinePTX\t    simpleMultiCopy\t\t  systemWideAtomics\n",
            "inlinePTX_nvrtc     simpleMultiGPU\t\t  template\n",
            "matrixMul\t    simpleOccupancy\t\t  UnifiedMemoryStreams\n",
            "matrixMulCUBLAS     simpleP2P\t\t\t  vectorAdd\n",
            "matrixMulDrv\t    simplePitchLinearTexture\t  vectorAddDrv\n",
            "matrixMul_nvrtc     simplePrintf\t\t  vectorAdd_nvrtc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rdnq00bro6i"
      },
      "source": [
        "# colab tip\n",
        "\n",
        "how to download a file to local?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2wrq6cjeuPa",
        "outputId": "b01f2f63-f8f9-4829-a653-7e6985a70edf"
      },
      "source": [
        "cd /usr/local/cuda/samples/0_Simple/clock"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/cuda-9.2/samples/0_Simple/clock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ujK1-KjPqZvM",
        "outputId": "a6644b33-bd96-4e9b-9108-d13d68f7619a"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('clock.cu')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e321e946-6ce6-4c17-b6a4-57bb7885760f\", \"clock.cu\", 4086)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}